# 深度学习

## 1 深度学习模型的构建 

### 1.1 模型类的创建

```python
import torch.nn as nn
class <moduleName>(nn.Module):
    def __init__(self,<params>):
        super(<moduleName>,self).__init__()
```

1. 引入`torch.nn`模块
2. 定义我们的模型类，并让其继承`nn.Module`模块
3. 定义初始化函数

### 1.2 模型参数的生成

1. 在`__init__()`函数中使用`nn.Parameter()`函数定义模型参数

   ```
   <paramName>=nn.Parameter（Tensor(<TensorSize>)）
   ```

2. 在`__init__()`函数中使用`nn.ParameterList(parameters=None)`来定义多个模型参数

   ```
   1.self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])
   ①我们可以通过self.params.append(nn.Parameter())的方法在ParameterList对象中添加参数
   ②我们可以通过self.params.extend(<ParameterObject>)的方法合并两个ParameterList对象
   ```

3. 在`__init__()`函数中使用`register_parameter(name, param)`的方式定义模型参数

   ```
   1.self.register_parameter(<name>,<TensorObject>)
   注意:以这种方式定义的模型参数可以通过self.name获取
   ```

4. 在`__init__()`函数中使用`Variable()`函数定义模型参数

   ```
   self.vari = Variable(torch.rand([1]))
   ```

==注意==:我们务必要使用上述两个方法来注册我们的模型参数,因为这样模型参数才能被正确注册,并通过`model.parameters()`获得模型参数的生成器,若不这样做,则会导致我们的模型训练十分难以实现.

### 1.3 模型参数初始化

#### 1.3.1 使用`nn.init`模块进行模型参数初始化

```
torch.nn.init.uniform(<param>, a=0, b=1):从均匀分布U(a, b)中生成值，填充输入的张量或变量

torch.nn.init.normal(<param>, mean=0, std=1):从给定均值和标准差的正态分布N(mean, std)中生成值，填充输入的张量或变量

torch.nn.init.constant(<param>, val):用val的值填充输入的张量或变量

torch.nn.init.eye(<param):用单位矩阵来填充2维输入张量或变量。在线性层尽可能多的保存输入特性。

torch.nn.init.dirac(<param>):用Dirac $\delta$ 函数来填充{3, 4, 5}维输入张量或变量。在卷积层尽可能多的保存输入通道特性。

torch.nn.init.xavier_uniform(<param>, gain=1):根据Glorot, X.和Bengio, Y.在“Understanding the difficulty of 		training deep feedforward neural networks”中描述的方法，用一个均匀分布生成值，填充输入的张量或变量。结果张量中的值采		样自U(-a, a)，其中a= gain * sqrt( 2/(fan_in + fan_out))* sqrt(3). 该方法也被称为Glorot initialisation

torch.nn.init.xavier_normal(<param>, gain=1)

torch.nn.init.kaiming_uniform(<param>, a=0, mode='fan_in')

torch.nn.init.kaiming_normal(<param>, a=0, mode='fan_in')

torch.nn.init.sparse(<param>, sparsity, std=0.01):将2维的输入张量或变量当做稀疏矩阵填充，其中非零元素根据一个均值为0，标	准差为std的正态分布生成。 参考Martens, J.(2010)的 “Deep learning via Hessian-free optimization”.
```



### 1.4 非模型参数变量的生成

1. 使用`register_buffer(name, tensor)`来进行非模型参数的保存

   ```
   应用场景:我们需要保存一个状态，但是这个状态不能看作成为模型参数。 例如：, BatchNorm’s running_mean 不是一个 parameter, 但是它也是需要保存的状态之一。
   1.register_buffer('running_mean', torch.zeros(num_features))
   self.running_mean
   注意:以这种方式定义的变量可以通过self.name获取
   ```

### 1.5 子模块导入

- `nn.Sequential()`导入

  ```
  1.model = nn.Sequential(
            nn.Conv2d(1,20,5),
            nn.ReLU(),
            nn.Conv2d(20,64,5),
            nn.ReLU()
          )
  2.model = nn.Sequential(OrderedDict([
            ('conv1', nn.Conv2d(1,20,5)),
            ('relu1', nn.ReLU()),
            ('conv2', nn.Conv2d(20,64,5)),
            ('relu2', nn.ReLU())
          ]))
  ```

- `nn.ModuleList()`导入

  ```
  1.self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])
  注意:
  ①我们可以通过self.linears.append(nn.Linear(10,10))的方式在ModuleList对象中添加模块
  ②我们还可以通过self.linears.extend(<ModuleListObject>)的方式合并两个ModuleList对象
  ```

- `add_Module()`导入

  ```
  1.self.add_module("conv", nn.Conv2d(10, 20, 4))
  注意:通过这种方法导入的子模块我们可以通过model.<ModuleName>来获取
  ```

- 直接实例化导入

  ```
  self.conv1 = nn.Conv2d(1, 20, 5)# submodule: Conv2d
  ```

==注意==:我们务必要用上述方法导入我们的子模块,只有这样才能让我们的子模块正确的被注册,从而正确的转递其参数到我们的主模块的`nn.parameters()`生成的迭代器中

1.6 模型参数在**CPU**与**GPU**之间转移

1. 使用`cpu()`方法将模型参数复制到`CPU`上

   ```
   model.cpu()
   ```

2. 使用`cuda()`方法将模型参数复制到`GPU`上

   ```
   model.cuda()
   ```

### 1.6 定义前向函数

```
def forward(input):
	
	前向语句体
	
	return output
```

### 1.7 模型训练

1. 实例化模型并将模型加载到**GPU**

   ```
   model = LeNet().cuda()
   ```

2. 实例化优化器

   ```
   optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)
   ```

3. 实例化损失函数并将损失函数加载到**GPU**上

   ```
   loss = nn.NLLLoss().cuda()
   ```

4. 将模型调整到训练状态

   ```
   model.train()
   作用:在我们的模型存在BatchNormalization/Dropout时使用,以开启这两个trick
   ```

5. 构建迭代循环

   1. device=torch.device('cuda')

   2. ==注意==:我们务必要把输入与标签都通过.to(device)的方法导入到GPU上

      ```
      input,label=input.to(device),label.to(device)
      ```

   3. 将输入导入模型获得输出

      ```
      output = model(input)
      ```

   4. 将输出与标签导入损失函数获取Loss

      ```
      loss = LossFunction(output,label)
      ```

6. 对优化器梯度进行清零操作

   ```
   optimizer.zero_grad()
   ```

7. 对损失对象求梯度

   ```
   loss.backward()
   ```

8. 对模型参数应用优化

   ```
   optimizer.step()
   ```

### 1.8 模型测试

1. 将模型调整到测试状态

   ```
   model.eval()
   作用:当模型存在BatchNormalization/Dropout机制时使用,以关闭这两个trick
   ```

2. 构建测试循环

   ```
   with torch.no_grad()用于将我们的测试过程排除梯度计算
   	测试循环
   注意:with torch.no_grad()用于将我们的测试过程排除梯度计算
   ```

   

### 1.9 模型保存与加载

1. 只保存与加载模型参数

   ```
   1.使用state_dict()获取模型参数的字典并保存
   
   	torch.save({'model': model.state_dict()}, 'SavePath/model_name.pth')
   
   2.使用load方法获取被保存的模型参数并加载
   
   	model = net()
   	state_dict = torch.load('model_name.pth')
   	model.load_state_dict(state_dict['model'])
   ```

2. 保存整个模型并读取

   ```
   ##假设这是我们的原有模型
   model = net()
   
   ## 保存模型
   torch.save(model, 'model_name.pth')
   
   ## 读取模型
   model = torch.load('model_name.pth')
   注意:在读取之前model变量是没有被实例化为模型对象的
   ```

   

## 2 自定义函数

### 2.1 为什么要自定义函数?

`Pytorch`中每一个基本数学运算都在底层有着`Pytorch`型的定义,他们都会定义有`backward()`方法,这个方法是`Pytorch`实现自动梯度求解的关键.值得我们注意的是当我们将多个`Pytorch`型运算组合使用时,是无需进行自定义函数编制的,只有当我们定义一种在`Pytorch`基本函数中==没有的新型数学运算函数==时,才需要进行自定义,若不进行自定义则会导致我们的该步运算==无法自动求解梯度==.



## 3 自定义数据集



## 4 数据加载

### 4.1 实例化采样器

```
class torch.utils.data.sampler.SequentialSampler(<DatasetObject>):样本元素顺序排列，始终以相同的顺序。

class torch.utils.data.sampler.RandomSampler(<DatasetObject>):样本元素随机，没有替换

class torch.utils.data.sampler.SubsetRandomSampler(<indices>):样本元素从指定的索引列表中随机抽取，没有替换。

class torch.utils.data.sampler.WeightedRandomSampler(weights, num_samples, replacement=True):样本元素来自于														[0,..,len(weights)-1]，给定概率（weights）。
```

### 4.2 使用`torch.utils.data.DataLoader()`进行数据加载

```
class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=<function default_collate>, pin_memory=False, drop_last=False)

参数介绍
	dataset (Dataset)             :加载数据的数据集。
	batch_size (int, optional)    :每个batch加载多少个样本(默认: 1)。
	shuffle (bool, optional)      :设置为True时会在每个epoch重新打乱数据(默认: False).
	sampler (Sampler, optional)   :定义从数据集中提取样本的策略。如果指定，则忽略shuffle参数。
	num_workers (int, optional)   :用多少个子进程加载数据。0表示数据将在主进程中加载(默认: 0)
	collate_fn (callable, optional)
	pin_memory (bool, optional)
	drop_last (bool, optional)    :如果数据集大小不能被batch size整除，则设置为True后可删除最后一个不完整的batch。								   如果设为False并且数据集的大小不能被batch size整除，则最后一个batch将更小。(默								   认: False)
```

### 4.3 定义==实例==

```
sampler = torch.utils.data.sampler.RandomSampler(<DatasetObject>)
Dataloader = torch.utils.data.DataLoader(dataset,batch_size=32,shuffle=false,sampler=sampler)
```

4.4 加载数据

```
for index,(input,label) in enumerate(<DataloaderObject>)
```



## 5 `nn.conv1d()`与`nn.functional.conv1d()`

​		`nn.conv1d`是被我们的模型作为子模块引入的,因此其内部使用到的参数也会注册到我们的主模块的参数列表中,也即其参数会参与梯度更新过程.但是`nn.functional.conv1d()`则是被作为函数引入的,因此其内部使用到的参数不会被注册到我们的主模块的参数列表中,也就不会进行梯度更新,一般来说我们对激活函数等不需要参数更新的模块使用`nn.functional`中的形式,而对需要参数更新的使用`nn`中的形式