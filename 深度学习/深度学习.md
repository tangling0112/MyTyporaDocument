# 深度学习

## 1 深度学习模型的构建 

### 1.1 模型类的创建

```python
import torch.nn as nn
class <moduleName>(nn.Module):
    def __init__(self,<params>):
        super(<moduleName>,self).__init__()
```

1. 引入`torch.nn`模块
2. 定义我们的模型类，并让其继承`nn.Module`模块
3. 定义初始化函数

### 1.2 模型参数的生成

1. 在`__init__()`函数中使用`nn.Parameter()`函数定义模型参数

   ```python
   <paramName>=nn.Parameter（Tensor(<TensorSize>)）
   ```

2. 在`__init__()`函数中使用`nn.ParameterList(parameters=None)`来定义多个模型参数

   ```python
   1.self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])
   ①我们可以通过self.params.append(nn.Parameter())的方法在ParameterList对象中添加参数
   ②我们可以通过self.params.extend(<ParameterObject>)的方法合并两个ParameterList对象
   ```

3. 在`__init__()`函数中使用`register_parameter(name, param)`的方式定义模型参数

   ```python
   1.self.register_parameter(<name>,<TensorObject>)
   注意:以这种方式定义的模型参数可以通过self.name获取
   ```

4. 在`__init__()`函数中使用`Variable()`函数定义模型参数

   ```python
   self.vari = Variable(torch.rand([1]))
   ```

==注意==:我们务必要使用上述两个方法来注册我们的模型参数,因为这样模型参数才能被正确注册,并通过`model.parameters()`获得模型参数的生成器,若不这样做,则会导致我们的模型训练十分难以实现.

### 1.3 模型参数初始化

#### 1.3.1 使用`nn.init`模块进行模型参数初始化

```python
torch.nn.init.uniform(<param>, a=0, b=1):从均匀分布U(a, b)中生成值，填充输入的张量或变量

torch.nn.init.normal(<param>, mean=0, std=1):从给定均值和标准差的正态分布N(mean, std)中生成值，填充输入的张量或变量

torch.nn.init.constant(<param>, val):用val的值填充输入的张量或变量

torch.nn.init.eye(<param):用单位矩阵来填充2维输入张量或变量。在线性层尽可能多的保存输入特性。

torch.nn.init.dirac(<param>):用Dirac $\delta$ 函数来填充{3, 4, 5}维输入张量或变量。在卷积层尽可能多的保存输入通道特性。

torch.nn.init.xavier_uniform(<param>, gain=1):根据Glorot, X.和Bengio, Y.在“Understanding the difficulty of 		training deep feedforward neural networks”中描述的方法，用一个均匀分布生成值，填充输入的张量或变量。结果张量中的值采		样自U(-a, a)，其中a= gain * sqrt( 2/(fan_in + fan_out))* sqrt(3). 该方法也被称为Glorot initialisation

torch.nn.init.xavier_normal(<param>, gain=1)

torch.nn.init.kaiming_uniform(<param>, a=0, mode='fan_in')

torch.nn.init.kaiming_normal(<param>, a=0, mode='fan_in')

torch.nn.init.sparse(<param>, sparsity, std=0.01):将2维的输入张量或变量当做稀疏矩阵填充，其中非零元素根据一个均值为0，标	准差为std的正态分布生成。 参考Martens, J.(2010)的 “Deep learning via Hessian-free optimization”.
```



### 1.4 非模型参数变量的生成

1. 使用`register_buffer(name, tensor)`来进行非模型参数的保存

   ```python
   应用场景:我们需要保存一个状态，但是这个状态不能看作成为模型参数。 例如：, BatchNorm’s running_mean 不是一个 parameter, 但是它也是需要保存的状态之一。
   1.register_buffer('running_mean', torch.zeros(num_features))
   self.running_mean
   注意:以这种方式定义的变量可以通过self.name获取
   ```

### 1.5 子模块导入

- `nn.Sequential()`导入

  ```python
  1.model = nn.Sequential(
            nn.Conv2d(1,20,5),
            nn.ReLU(),
            nn.Conv2d(20,64,5),
            nn.ReLU()
          )
  2.model = nn.Sequential(OrderedDict([
            ('conv1', nn.Conv2d(1,20,5)),
            ('relu1', nn.ReLU()),
            ('conv2', nn.Conv2d(20,64,5)),
            ('relu2', nn.ReLU())
          ]))
  ```

- `nn.ModuleList()`导入

  ```python
  1.self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])
  注意:
  ①我们可以通过self.linears.append(nn.Linear(10,10))的方式在ModuleList对象中添加模块
  ②我们还可以通过self.linears.extend(<ModuleListObject>)的方式合并两个ModuleList对象
  ```

- `add_Module()`导入

  ```python
  1.self.add_module("conv", nn.Conv2d(10, 20, 4))
  注意:通过这种方法导入的子模块我们可以通过model.<ModuleName>来获取
  ```

- 直接实例化导入

  ```python
  self.conv1 = nn.Conv2d(1, 20, 5)# submodule: Conv2d
  ```

==注意==:我们务必要用上述方法导入我们的子模块,只有这样才能让我们的子模块正确的被注册,从而正确的转递其参数到我们的主模块的`nn.parameters()`生成的迭代器中

1.6 模型参数在**CPU**与**GPU**之间转移

1. 使用`cpu()`方法将模型参数复制到`CPU`上

   ```python
   model.cpu()
   ```

2. 使用`cuda()`方法将模型参数复制到`GPU`上

   ```python
   model.cuda()
   ```

### 1.6 定义前向函数

```python
def forward(input):
	
	前向语句体
	
	return output
```

### 1.7 模型训练

1. 实例化模型并将模型加载到**GPU**

   ```python
   model = LeNet().cuda()
   ```

2. 实例化优化器

   ```python
   optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)
   ```

3. 实例化损失函数并将损失函数加载到**GPU**上

   ```python
   loss = nn.NLLLoss().cuda()
   ```

4. 将模型调整到训练状态

   ```python
   model.train()
   作用:在我们的模型存在BatchNormalization/Dropout时使用,以开启这两个trick
   ```

5. 构建迭代循环

   1. device=torch.device('cuda')

   2. ==注意==:我们务必要把输入与标签都通过.to(device)的方法导入到GPU上

      ```python
      input,label=input.to(device),label.to(device)
      ```

   3. 将输入导入模型获得输出

      ```python
      output = model(input)
      ```

   4. 将输出与标签导入损失函数获取Loss

      ```python
      loss = LossFunction(output,label)
      ```

6. 对优化器梯度进行清零操作

   ```python
   optimizer.zero_grad()
   ```

7. 对损失对象求梯度

   ```python
   loss.backward()
   ```

8. 对模型参数应用优化

   ```python
   optimizer.step()
   ```

### 1.8 模型测试

1. 将模型调整到测试状态

   ```python
   model.eval()
   作用:当模型存在BatchNormalization/Dropout机制时使用,以关闭这两个trick
   ```

2. 构建测试循环

   ```python
   with torch.no_grad()用于将我们的测试过程排除梯度计算
   	测试循环
   注意:with torch.no_grad()用于将我们的测试过程排除梯度计算
   ```

   

### 1.9 模型保存与加载

1. 只保存与加载模型参数

   ```python
   1.使用state_dict()获取模型参数的字典并保存
   
   	torch.save({'model': model.state_dict()}, 'SavePath/model_name.pth')
   
   2.使用load方法获取被保存的模型参数并加载
   
   	model = net()
   	state_dict = torch.load('model_name.pth')
   	model.load_state_dict(state_dict['model'])
   ```

2. 保存整个模型并读取

   ```python
   ##假设这是我们的原有模型
   model = net()
   
   ## 保存模型
   torch.save(model, 'model_name.pth')
   
   ## 读取模型
   model = torch.load('model_name.pth')
   注意:在读取之前model变量是没有被实例化为模型对象的
   ```

   

## 2 自定义函数

### 2.1 为什么要自定义函数?

`Pytorch`中每一个基本数学运算都在底层有着`Pytorch`型的定义,他们都会定义有`backward()`方法,这个方法是`Pytorch`实现自动梯度求解的关键.值得我们注意的是当我们将多个`Pytorch`型运算组合使用时,是无需进行自定义函数编制的,只有当我们定义一种在`Pytorch`基本函数中==没有的新型数学运算函数==时,才需要进行自定义,若不进行自定义则会导致我们的该步运算==无法自动求解梯度==.



## 3 自定义数据集

### 3.1 自定义英中翻译数据集

#### 3.1.1 定义文本标识符

```python
pad_token = "<pad>"#用于填充字符串从而让每一个输入等长
unk_token = "<unk>"#用于当我们的单词没有存在我们的词典中时，标志该单词为未知单词
bos_token = "<bos>"#用于标识句子的开始
eos_token = "<eos>"#用于标识句子的结束

extra_tokens = [pad_token, unk_token, bos_token, eos_token]

PAD = extra_tokens.index(pad_token)
UNK = extra_tokens.index(unk_token)
BOS = extra_tokens.index(bos_token)
EOS = extra_tokens.index(eos_token)
```

#### 3.1.2 定义单词转化索引函数

```python
def convert_text2idx(self.sampels,word2inx):
    '''
    examples:一个容纳了数据集内所有句子的二维数组,每一行为一个句子
    word2idx:3.1.6函数的返回值之一
    '''
    idx_text = []#用于容纳整个文本数据集的用词典索引替代单词的二维数组,每一行为数据集中的一句话
    idx_line = []#用于容纳单个句子的用词典索引代替单词的一维数组
    for sent in examples:
        idx_line.append(self.BOS)#在句子首部添加BOS开始符号
        for w in sent:
            if w in word2idx:
                idx_line.append(word2idx[w])
            else:
                idx_line.append(self.UNK)
        idx_line.append(self.EOS)#在句子尾部添加EOS句子结束符号
        idx_text.append(idx_line)
        idx_line = []
    return idx_text

```

#### 3.1.3 定义索引转化单词函数

```python
def convert_idx2text(self,example, idx2word):
    '''
    example:一个有单词索引组成的一维数组
    idx2word:3.1.6的返回值之一
    '''
    words = []
    for i in example:
        if i == self.EOS:#当遇到结束标识符时终止
            break
        if i== self.BOS:
            continue
        words.append(idx2word[i])#将该索引对应的单词
    return ' '.join(words)#将单词数组以每个元素之间相隔一个空白符的规则转化为字符串
```

#### 3.1.4 定义`padding`添加函数

```python
def add_padding(self,idx_text,max_len):
    '''
    idx_text:已经转化为词典索引标识的代表数据集的二维数组,每一行表示一个句子
    max_len:指示最大句子包含词语数,如果小于该值,则添加一定个数PAD符号填充到max_len长度
    '''
    PADadded_idx_text = []
    for idx_line in idx_text:
        if len(idx_line) == max_len:
            continue
        elif len(idx_line) < max_len:
            for i in range(max_len-len(idx_line)):
                idx_line.append(self.PAD)
        PADadded_idx_text.append(idx_line)
    return PADadded_idx_text#已经添加了PAD的代表数据集的二维索引值数组
```



#### 3.1.5 定义英文数据集读取函数

**基本步骤**

1. 使用`with…open…`读取文件
2. 使用`enumerate`迭代文件的每一行
3. 去除空行
4. 去除长度过大的行

```python
def read_en_corpus(self,src_path, max_len, lower_case=False):
   	'''src_path:数据集保存的文件地址
   	   max_len:读取的一句话可以包含的最多单词数
   	   lower_case:用于指定是否将文本转化为小写
   	'''
    src_sents = []
    empty_lines, exceed_lines = 0, 0
    with open(src_path) as src_file:
        for idx, src_line in enumerate(src_file):
            if idx % 10000 == 0:
                print('  reading {} lines..'.format(idx))
            if src_line.strip() == '':  #当遇到空行时，让empty_lines加1并直接跳过这一行
                empty_lines += 1
                continue
            if lower_case:  #用于对文本进行小写转换
                src_line = src_line.lower()

            src_words = src_line.strip().split()
            if max_len is not None and len(src_words) > max_len:#判断指定行包含的单词数量是否超过max_len如果超过则跳过该行，不将其录入
                exceed_lines += 1
                continue
            src_sents.append(src_words)#添加读取到的行到src_sents数组中形成二维数组
    return src_sents
```

#### 3.1.5 定义中文数据集读取函数

```python
import jieba
def read_zh_corpus(self,src_path, max_len):
   	'''src_path:数据集保存的文件地址
   	   max_len:读取的一句话可以包含的最多单词数
   	'''
    src_sents = []
    empty_lines, exceed_lines = 0, 0
    with open(src_path) as src_file:
        for idx, src_line in enumerate(src_file):
            if idx % 10000 == 0:
                print('  reading {} lines..'.format(idx))
            if src_line.strip() == '':  #当遇到空行时，让empty_lines加1并直接跳过这一行
                empty_lines += 1
                continue
            src_words = jieba.cut(src_line.strip())#使用jieba库的精确搜索模式对句子进行词语拆分
            if max_len is not None and len(src_words) > max_len:#判断指定行包含的单词数量是否超过max_len如果超过则跳过该行，不将其录入
                exceed_lines += 1
                continue
            src_sents.append(src_words)#添加读取到的行到src_sents数组中形成二维数组
    return src_sents
```

#### 3.1.7 定义英文单词词典建立函数

```python
import Counter
def build_En_vocab(self,examples, max_size, min_freq, extra_tokens):
    '''examples:一个包含着英文数据集内所有文本的二维数组,每一行,为一句话
       max_size:定义词典最大可包含的单词的数量
       min_freq:用于指定一个单词的最小出现次数,如果小于该值,则这个词不会被添加到字典中
       extra_tokens:一个存储着四个标识符的数组
    '''
    counter = Counter()#一个计数器实例,来自于Counter库
    word2idx, idx2word = {}, []
    '''将四个标识符添加到字典中'''
    if extra_tokens:
        idx2word += extra_tokens
        word2idx = {word: idx for idx, word in enumerate(extra_tokens)}
        
    min_freq = max(min_freq, 1)
    max_size = max_size + len(idx2word) if max_size else None#由于扩展到四个标识符不算到词典最大长度中,因此将max_size扩大
    for sent in examples:#迭代examples数组的每一行,也即数据集的每一句话.
        for w in sent:#迭代sent数组的每一个元素,也即一句话中的每一个单词
            counter.update([w])
    sorted_counter = sorted(counter.items(), key=lambda tup: tup[0])#按照单词词序进行排列
    '''
    每一个counter的item都是一个含有两个元素的数组,第一个元素指的是被添加到counter实例的单词,第二个为其出现的次数
    '''
    sorted_counter.sort(key=lambda tup: tup[1], reverse=True)#再按照单词出现的频率从大到小排序

    for word, freq in sorted_counter:
        if freq < min_freq or (max_size and len(idx2word) == max_size):
            break
            
        idx2word.append(word)#将单词添加到idx2word数组中
        word2idx[word] = len(idx2word) - 1
        '''
        1.idx2word数组中添加一个单词,此时长度为n+1
        2.在word2idx字典中以单词字符串为键,该单词在idx2word中的索引值为值添加
        '''
        
    return counter, word2idx, idx2word
	'''
	counter:我们的counter实例
	word2idx:单词为键,单词在idx2word中的索引为值的字典对象
	idx2word:保存了词典中所有单词的二维数组,每一行为一个单词
	'''
```

#### 3.1.8 借助`jieba`库定义中文词语词典建立函数

```python
import Counter
import jieba
def buld_zh_vocab(self,examples,max_size,min_freq,extra_tokens)
	'''
	examples:一个包含着中文数据集内所有文本的二维数组,每一行,为一句话
	max_size:定义词典最大可包含的词语的数量
	min_freq:用于指定一个词语的最小出现次数,如果小于该值,则这个词不会被添加到词典中
	extra_tokens:一个存储着四个标识符的数组
	'''
    counter = Counter()#一个计数器实例,来自于Counter库
    word2idx, idx2word = {}, []
    '''将四个标识符添加到字典中'''
    if extra_tokens:
        idx2word += extra_tokens
        word2idx = {word: idx for idx, word in enumerate(extra_tokens)}
        
    min_freq = max(min_freq, 1)
    max_size = max_size + len(idx2word) if max_size else None#由于扩展到四个标识符不算到词典最大长度中,因此将max_size扩大
    for sent in examples:#迭代examples数组的每一行,也即数据集的每一句话.
        for w in sent:#迭代sent数组的每一个元素,也即一句话中的每一个单词
            counter.update([w])
    sorted_counter = sorted(counter.items(), key=lambda tup: tup[0])#按照单词词序进行排列
    '''
    每一个counter的item都是一个含有两个元素的数组,第一个元素指的是被添加到counter实例的单词,第二个为其出现的次数
    '''
    sorted_counter.sort(key=lambda tup: tup[1], reverse=True)#再按照单词出现的频率从大到小排序

    for word, freq in sorted_counter:
        if freq < min_freq or (max_size and len(idx2word) == max_size):
            break
            
        idx2word.append(word)#将单词添加到idx2word数组中
        word2idx[word] = len(idx2word) - 1
        '''
        1.idx2word数组中添加一个单词,此时长度为n+1
        2.在word2idx字典中以单词字符串为键,该单词在idx2word中的索引值为值添加
        '''
        
    return counter, word2idx, idx2word
	'''
	counter:我们的counter实例
	word2idx:单词为键,单词在idx2word中的索引为值的字典对象
	idx2word:保存了词典中所有单词的二维数组,每一行为一个单词
	'''
```



#### 3.1.9 定义数据集类

```python
from torch.utils.data import Dataset
class MyDataset(Dataset):
    def __init__(self,trainning='train'):
        '''定义文本标识符'''
        pad_token = "<pad>"#用于填充字符串从而让每一个输入等长
		unk_token = "<unk>"#用于当我们的单词没有存在我们的词典中时，标志该单词为未知单词
		bos_token = "<bos>"#用于标识句子的开始
		eos_token = "<eos>"#用于标识句子的结束

		self.extra_tokens = [pad_token, unk_token, bos_token, eos_token]

		self.PAD = self.extra_tokens.index(pad_token)
		self.UNK = self.extra_tokens.index(unk_token)
		self.BOS = self.extra_tokens.index(bos_token)
		self.EOS = self.extra_tokens.index(eos_token)
        '''读取文件获取数据集组成的二维向量'''
        self.examples_zh = self.read_zh_corpus(<path>,<max_len>)
        self.examples_en = self.read_en_corpus(<path>,<max_len>)
       '''使用数据集的二维向量构建词典'''
    	self.counter_en,self.word2idx_en,self.idx2word_en = self.build_En_vocab(self.examples_en, <max_size>,<min_freq>, <extra_tokens>)
        self.counter_zh,self.word2idx_zh,self.idx2word_zh = self.build_zh_vocab(self.examples_zh, <max_size>,<min_freq>, <extra_tokens>)
        '''使用词典将原数据集组成的二维向量转化成由字典索引组成的二维向量'''
        self.examples_en_idx = self.convert_text2idx(self.examples_en,word2idx_en)
        self.examples_zh_idx = self.convert_text2idx(self.examples_zh,word2idx-zh)
        self.vocab_size_zh = len(idx2word_zh)
        self.vocab_size_en = len(idx2word_en)
        
        if trainning=='train':
            self.en_text_idx = examples_en_idx[0:6000]
            self.zh_text_idx = examples_zh_idx[0:6000]
        elif trainning=='valid':
            self.en_text_idx = examples_en_idx[6001:8000]
            self.zh_text_idx = examples_zh_idx[6001:8000]
        else:
            self.en_text_idx = examples_en_idx[8001:10000]
            self.zh_text_idx = examples_zh_idx[8001:10000]
 
    
    def __len__(self):
        return self.en_text_idx.shape[0]
   

	def __getitem__(self,idx):
        return self.add_padding(en_text_idx[idx],<max_len>),self.add_padding(zh_text_idx[idx],<max_len>)
   	
    
    def get_vocab_size(self):
        return self.vocab_size_en,self.vocab_size_zh
```

#### 3.1.10 使用nn.Embedding实现词向量

### 3.2 自定义图像分类数据集



## 4 数据加载

### 4.1 实例化采样器

```
class torch.utils.data.sampler.SequentialSampler(<DatasetObject>):样本元素顺序排列，始终以相同的顺序。

class torch.utils.data.sampler.RandomSampler(<DatasetObject>):样本元素随机，没有替换

class torch.utils.data.sampler.SubsetRandomSampler(<indices>):样本元素从指定的索引列表中随机抽取，没有替换。

class torch.utils.data.sampler.WeightedRandomSampler(weights, num_samples, replacement=True):样本元素来自于														[0,..,len(weights)-1]，给定概率（weights）。
```

### 4.2 使用`torch.utils.data.DataLoader()`进行数据加载

```
class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=<function default_collate>, pin_memory=False, drop_last=False)

参数介绍
	dataset (Dataset)             :加载数据的数据集。
	batch_size (int, optional)    :每个batch加载多少个样本(默认: 1)。
	shuffle (bool, optional)      :设置为True时会在每个epoch重新打乱数据(默认: False).
	sampler (Sampler, optional)   :定义从数据集中提取样本的策略。如果指定，则忽略shuffle参数。
	num_workers (int, optional)   :用多少个子进程加载数据。0表示数据将在主进程中加载(默认: 0)
	collate_fn (callable, optional)
	pin_memory (bool, optional)
	drop_last (bool, optional)    :如果数据集大小不能被batch size整除，则设置为True后可删除最后一个不完整的batch。								   如果设为False并且数据集的大小不能被batch size整除，则最后一个batch将更小。(默								   认: False)
```

### 4.3 定义==实例==

```
sampler = torch.utils.data.sampler.RandomSampler(<DatasetObject>)
Dataloader = torch.utils.data.DataLoader(dataset,batch_size=32,shuffle=false,sampler=sampler)
```

4.4 加载数据

```
for index,(input,label) in enumerate(Dataloader)
```



## 5 `nn.conv1d()`与`nn.functional.conv1d()`

​		`nn.conv1d`是被我们的模型作为子模块引入的,因此其内部使用到的参数也会注册到我们的主模块的参数列表中,也即其参数会参与梯度更新过程.但是`nn.functional.conv1d()`则是被作为函数引入的,因此其内部使用到的参数不会被注册到我们的主模块的参数列表中,也就不会进行梯度更新,一般来说我们对激活函数等不需要参数更新的模块使用`nn.functional`中的形式,而对需要参数更新的使用`nn`中的形式