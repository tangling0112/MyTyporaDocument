# 视频理解

> **视频理解**即对于视频中出现的人或物的行为或者状态等进行理解,并以文字的形式向我们反馈出这些情形的描述性语言
>
> **问题**:对于图片数据的理解而言,常见的思路为直接将图片输入我们的卷积神经网络,然后将其认为是一个分类问题进行处理,但是对于**视频数据**而言,这中思路是不完全正确的,首先一个视频是由大量图片帧组成的,图片数据量极大,其次,视频中相邻的几帧图片之间的差异不会很大,我们实际上无需对这视频的每一帧进行处理.因此这也就产生了一些采用关键帧识别等技巧.

## 视频理解算法的发展

### 卷积+LSTM

- <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510103447470.png" alt="image-20220510103447470" style="zoom: 80%;" />

### 3D-卷积

- <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510103536422.png" alt="image-20220510103536422" style="zoom:80%;" />

### 双流网络

- <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510103556766.png" alt="image-20220510103556766" style="zoom:80%;" />

### 3D-Fused双流网络

- <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510103629839.png" alt="image-20220510103629839" style="zoom:80%;" />

### 3D-卷积的双流网络

- <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510103704057.png" alt="image-20220510103704057" style="zoom:80%;" />

## 双流卷积神经网络

### 空间流卷积神经网络

- **输入**:一系列普通的图片

- <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220509122617362.png" alt="image-20220509122617362" style="zoom:67%;" />

### 时间流卷积神经网络

- **输入**:一系列的**光流**的图片

- <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220509122631377.png" alt="image-20220509122631377" style="zoom: 67%;" />

### 解决的问题

- 传统的卷积神经网络是无法综合理解视频中物体的运动特征的,其只能处理静态的图像.而双流卷积神经网络则旨在通过对卷积神经网络结构的特殊设计(让网络同时利用**普通图片**信息与**光流图像**信息),让我们的神经网络能够处理视频中物体的运动特征
- 过去的一部分工作并没有在结构上很好地利用视频数据的视频帧之间时序信息,而这导致这些工作的最终表现甚至远远低于巧妙地编程设计的特征提取方法,而双流神经网络注意到了这一点,其巧妙地设计网络结构,从而让视频数据的视频帧之间的时序信息得到了充分的利用

### 缺点

- 由于光流图像的抽取耗时,导致模型的训练耗时长,实际应用也不能满足实时要求

### 光流

> 一个$$100$$帧的输入视频得到的输出为$99$帧的光流图

![image-20220509130012562](C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220509130012562.png)

- 光流的定义

- 光流的前提假设
  - 光线沿着直线行进
  - 光线与光线即便相遇也不会发生碰撞
  - 光线会在场景中不断反射然后进入人的眼睛，然后成像
  - 如果我们可以接收到一个物体反射过来的光线，那么该物体也同样能够接收到你反射出来的光线

- 光流的基本特性
- 光流的抽取(基于``OpenCV``)
- 光流的缺点
  - 要计算一帧光流结果的耗时较长


### 基本思想

> - 假设视频为$2400×1600×3$的尺寸,每个视频有$N+1$帧
> - 并不是所有的视频帧图像都要进入我们的空间卷积神经网络,而是给出了一个间隔值,每隔固定间隔值从$L+1$帧中按顺序抽取一帧,作为空间卷积神经网络的输入,

#### 空间卷积神经网络

- 输入视频的第`k`帧$2400×1600×3$的视频帧图片,`resize`为$224×224×3$的图片
- 进行卷积神经网络的前向计算
- 得到`SoftMax`后的输出`S`

#### 时间卷积神经网络

- 对视频进行光流估计,得到$2400×1600×2$大小(第一个通道为水平方向的光流估计,第二个通道为竖直方向的光流估计)共$N$帧的光流估计图像
- 将当前空间卷积神经网络输入视频帧以及其后$L-1$帧共$L$个光流估计图像简单的叠加为$2400×1600×2L$的时间卷积神经网络的输入
- 进行卷积神经网络的前向计算
- 得到`SoftMax`后的输出`T`

### 模型架构

- ​	![image-20220509124405086](C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220509124405086.png)

### 模型测试的细节

- 帧抽取:在测试时,对于任何长度的输入视频,都等间距抽取共$25$帧,然后对这25帧的每一帧进行数据增强处理(每一帧扩充出10帧),因此对于每一个视频,我们有$$250$$帧空间图像需要处理
  - 分别以四个顶点为抽取的顶点取四个同样大小的帧
  - 再在帧的正中心取与前面四帧同大小的帧
  - 对原始帧做翻转操作后,再按照上述方法抽取五帧
  - 对于一张原始帧,共得到10个视频帧

### 光流数据处理的细节

## `I3D`网络

> 前提:在`I3D`之前也有使用`3D`卷积的论文,但是其遇到的问题为,由于模型过于庞大(涉及到三维数据处理)因此训练速度很慢.而`I3D`通过引入`Inflated`机制,很好地解决了这一问题,并同时通过使用`光流信息`提升了模型性能,并且还证明了直接使用基石网络(``ResNet50``,``VGG``,``Inception``等)的预训练权重作为光流信息处理以及空间流信息处理的可行性.(即我们的模型一开始不随机初始化参数,而是使用基石网络已经预训练好的参数进行模型参数初始化.)

### 主要贡献

- 建立了一个新的多达$30$万个视频的新的数据集`Kinetic`数据集
- 建立了一个新的模型,并通过在`Kinetic`训练集预训练在`UCF-101`数据集上达到$98$%的准确率
- :red_circle:引入`Inflated`与`Bootstraping`来使用`2D网络`的预训练参数来初始化`3D网络`的模型参数
- 证明了使用`2D网络`的预训练参数来初始化`3D网络`的参数的可行性

### 主要结构组成

- **``3D-ConvNet``**

  - 即三维卷积神经网络，其输入为`[B,T,C,H,W]=[批数量,时间帧,通道数,高,宽]`的多批量的一段视频,一个三维卷积神经网络的卷积,池化都为三维卷积与三维池化
  - 三维卷积

- **`Inflated`操作**

  - **作用**:即对于我们使用的基石网络(如``Resnet50,VGG16``),我们将其中使用的所有的``2D卷积,2D池化``转换为``3D卷积,3D池化``操作.而对于激活层,批规范化层都不进行改变
  - **基本思路**:
  - **实现方法**:

- **3D卷积神经网络的参数初始化**

  - 实质:将``2D卷积核``在时间维度上复制一定次数得到3D卷积核,然后对得到的``3D卷积核``整体除以我们的复制次数,池化层也是同理

  - ```python
    def init_weights(self, ctx):
            """该函数的作用为使用预训练好的2D卷积神经网络的参数,来对3D神经网络的参数进行初始化"""
    
            self.first_stage.initialize(ctx=ctx)
            self.res_layers.initialize(ctx=ctx)
            self.head.initialize(ctx=ctx)
    
            if self.pretrained_base and not self.pretrained:
                if self.depth == 50:
                    resnet2d = resnet50_v1b(pretrained=True)
                elif self.depth == 101:
                    resnet2d = resnet101_v1b(pretrained=True)
                else:
                    print('No such 2D pre-trained network of depth %d.' % (self.depth))
    			#获取到2D模型的权重参数
                weights2d = resnet2d.collect_params()
                if self.nonlocal_cfg is None:
                    weights3d = self.collect_params()
                else:
                    train_params_list = []
                    raw_params = self.collect_params()
                    for raw_name in raw_params.keys():
                        if 'nonlocal' in raw_name:
                            continue
                        train_params_list.append(raw_name)
                    init_patterns = '|'.join(train_params_list)
                    weights3d = self.collect_params(init_patterns)
                assert len(weights2d.keys()) == len(weights3d.keys()), 'Number of parameters should be same.'
    			#
                dict2d = {}
                for key_id, key_name in enumerate(weights2d.keys()):
                    dict2d[key_id] = key_name
    
                dict3d = {}
                for key_id, key_name in enumerate(weights3d.keys()):
                    dict3d[key_id] = key_name
    
                dict_transform = {}
                for key_id, key_name in dict3d.items():
                    dict_transform[dict2d[key_id]] = key_name
    
                cnt = 0
                for key2d, key3d in dict_transform.items():
                    if 'conv' in key3d:
                        temporal_dim = weights3d[key3d].shape[2]
                        temporal_2d = nd.expand_dims(weights2d[key2d].data(), axis=2)
                        inflated_2d = nd.broadcast_to(temporal_2d, shape=[0, 0, temporal_dim, 0, 0]) / temporal_dim
                        assert inflated_2d.shape == weights3d[key3d].shape, 'the shape of %s and %s does not match. ' % (key2d, key3d)
                        weights3d[key3d].set_data(inflated_2d)
                        cnt += 1
                        print('%s is done with shape: ' % (key3d), weights3d[key3d].shape)
                    if 'batchnorm' in key3d:
                        assert weights2d[key2d].shape == weights3d[key3d].shape, 'the shape of %s and %s does not match. ' % (key2d, key3d)
                        weights3d[key3d].set_data(weights2d[key2d].data())
                        cnt += 1
                        print('%s is done with shape: ' % (key3d), weights3d[key3d].shape)
                    if 'dense' in key3d:
                        cnt += 1
                        print('%s is skipped with shape: ' % (key3d), weights3d[key3d].shape)
    
                assert cnt == len(weights2d.keys()), 'Not all parameters have been ported, check the initialization.'
    ```

    

### 基本结构

<img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510104049807.png" alt="image-20220510104049807" style="zoom:80%;" />

### 架构示例

<img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510171817293.png" alt="image-20220510171817293" style="zoom: 50%;" />

## `Convolutional Two-Stream Network Fusion`(双流网络的后续工作)

## `TSN(Temporal Segment Network)`

### 基本思路

- 

### 基本结构

<img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510135633014.png" alt="image-20220510135633014" style="zoom:80%;" />

## `3D convolutional NetWork`

### 基本架构

- ```
  [16×112×112]=>[16×112×112]    Conv with Padding
  [16×112×112]=>[16×56×56]	  Pooling1
  [16×56×56]=>[16×56×56]		  Conv With Padding
  [16×56×56]=>[8×28×28]		  Pooling2
  [8×28×28]=>[8×28×28]		  Conv With Padding
  [8×28×28]=>[8×28×28]		  Conv With Padding
  [8×28×28]=>[4×14×14]		  Pooling3
  [4×14×14]=>[4×14×14]		  Conv With Padding
  [4×14×14]=>[4×14×14]		  Conv With Padding
  [4×14×14]=>[2×7×7]		  	  Pooling4
  [2×7×7]=>[2×7×7]			  Conv With Padding
  [2×7×7]=>[2×7×7]			  Conv With Padding
  [16×7×7]=>[1×7×7]			  Pooling5
  
  ```

  ![image-20220510170008220](C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510170008220.png)

  <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510171234651.png" alt="image-20220510171234651" style="zoom:67%;" />

### 三维卷积

> 前提:我们知道,二维卷积具备在我们通俗理解上的滑动窗口计算方法,还有实现基础上的基于特殊构造的矩阵的矩阵相乘计算方法,那么要实现三维卷积,必然也具备这两种层次上的计算方法.

- 直观计算方法
- 矩阵形式计算方法

## `Non-Local Neural	NetWorks`

### 主要贡献

- 将`Self-Attention`操作引入了视频理解领域,设计了以视频帧为单元的自注意计算流程

### `Self-Attention For Video`

- 架构图
  - <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510172640725.png" alt="image-20220510172640725" style="zoom:50%;" />

## `R(2D+1D)模型`

### 基本思路

### 结构对比

- <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510175148643.png" alt="image-20220510175148643" style="zoom: 50%;" />

### 基本结构

## `Slow Fast Networks`

### 主要贡献

### 基本思路

### 模型架构概览

- <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510175341194.png" alt="image-20220510175341194" style="zoom:67%;" />

### 模型具体组成

- <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510175520338.png" alt="image-20220510175520338" style="zoom: 67%;" />

### `Timesformer`

### 主要贡献

### 基本结构

### 基本思想

## 视频理解综述

### 融合机制介绍

- `Single Frame`:即对视频进行一帧一帧的处理
- `Late Fusion`:即对视频的多个帧做相同的卷积神经网络过程,然后将不同对视频帧的输出结合起来
- `Early Fusion`:即将视频的多个帧直接作为3D卷积神经网络的输入,然后得到输出
- `Slow Fusion`:即对视频的多个帧先进行卷积神经网络过程,然后将输出经过组合再次进入另一个卷积神经网络,以此类推

- ![image-20220510124940899](C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510124940899.png)

#### `Early Fusion`的常见操作

##### 融合方法

- `Max Fusion`:即对于两个要融合的特征图,将两个特征图的元素两两配对,然后在每一对中取两者之中的最大值作为我们融合的输出,最终得到一个和输入的两个特征图大小相同的输出
- `Concatenation Fusion`:即对于两个要融合的特征图,直接把这两个特征图以某种方式拼接起来(如通道拼接``C×H×W Concat C×H×W => 2C×H×W``等)
- `Conv Fusion`:即对于要融合的两个特征图,把它们按照通道拼接起来(`C×H×W Concat C×H×W => 2C×H×W`),然后对拼接的结果做卷积操作,得到`C×H×W`的输出
- `Sum Fusion`:即对与两个要融合的相同大小的特征图,直接把他们的元素两两配对,然后配对的两个元素相加,得到最终的输出,该输出与输入大小相同
- `Bilinear Fusion`
  - <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510134043685.png" alt="image-20220510134043685" style="zoom: 50%;" />

##### 融合时机设计

- <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510134318343.png" alt="image-20220510134318343" style="zoom:67%;" /><img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510134601610.png" alt="image-20220510134601610" style="zoom:50%;" />

## 使用``Self-Attention``做特征融合

> **目的**:以每个视频帧经过卷积神经网络的输出作为一个单元,按照时间顺序排列这些输出.然后对输出进行``Self-Attention``机制计算来进行时序信息的综合,从而让模型能够通过``Self-Attention``机制处理时序信息的能力让模型能够综合不同视频帧的信息再做出一个最终输出

- `Self-Attention`的基本思路
- `Self-Attention`的基本结构
  - <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510172946662.png" alt="image-20220510172946662" style="zoom:50%;" />
- `Self-Attention`应用在特征融合的问题
  - **如何对一个每个单元为一个$H×W$的视频帧,而不是一维向量的单词的词嵌入的问题设计一个求解视频帧之间相关性的二维矩阵的计算方式?**
    - `NLP`中`Self-Attention`的每一个单元为一个单词,而一个单词为一个`Embedding_Dim`维的向量,而在我们的视频处理中,`Self-Attention`的一个单元变为了一个视频帧的特征图,为一个$H×W$维的二维矩阵,那么如何改造`Self-Attention`的计算过程使得其能够在以二维矩阵为单元的情况下计算出一个反映各个视频帧之间权重的二维矩阵就是一个问题
  - **利用二维权重矩阵如何设计运算求解得到`Self-Attention`的结果?**
    - 当我们得到表示视频帧之间的相关性的二维矩阵后,如何把原本只需要将`Seq_len,E_dim`的句子表示矩阵与相关性矩阵简单地相乘就可以得到`Self-Attention`的结果的简单步骤,转化为将`Time×C×H×W`的四维张量与该二维矩阵的简单相乘就能得到`Self-Attention`的输出的计算

## 使用``LSTM``做特征融合

> **目的**:以每个视频帧经过卷积神经网络的输出作为一个单元,按照时间顺序排列这些输出.然后对输出进行多层``LSTM``网络进行时序信息综合,从而让模型能够通过``LSTM``处理时序信息的能力让模型能够综合不同视频帧的信息再做出一个最终输出

### 结构示例

- <img src="C:\Users\Administrator\Desktop\Typora文档\深度学习\视频理解.assets\image-20220510132537721.png" alt="image-20220510132537721" style="zoom: 67%;" />